{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import every significant libraries to do this study"
      ],
      "metadata": {
        "id": "DaI5fEmVO_dO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the other packages\n",
        "!pip install -U kaleido\n",
        "!pip install pyLDAvis"
      ],
      "metadata": {
        "id": "qIPOvAWfcEi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DzJXpG0iyrDo"
      },
      "outputs": [],
      "source": [
        "# Data analysis\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.special import jv\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from pprint import pprint\n",
        "from wordcloud import WordCloud\n",
        "import warnings\n",
        "# Text analysis\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import words\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "# TF-IDF Vectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import PCA\n",
        "# Clustering models\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn import metrics\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.metrics import davies_bouldin_score\n",
        "from scipy.spatial.distance import cdist\n",
        "# LDA topica modelling\n",
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "import tqdm\n",
        "from gensim.models import CoherenceModel\n",
        "import spacy\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "import pyLDAvis.gensim\n",
        "import pickle\n",
        "import pyLDAvis\n",
        "# Sentiment Analysis\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from tqdm.notebook import tqdm\n",
        "import plotly.express as px\n",
        "\n",
        "# Download text mining packages\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('words')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('vader_lexicon')\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "stopwords = stopwords.words('english')\n",
        "stopwords.extend(['from', 'subject', 're', 'edu', 'use'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import dataset"
      ],
      "metadata": {
        "id": "7YvowELXtrrf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "# df = pd.read_excel('./tiktok_df.xlsx')"
      ],
      "metadata": {
        "id": "_G6HuDx5yv-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel('./tiktok_df.xlsx')\n",
        "# Select only unique data\n",
        "selected_df = df.drop_duplicates(subset='id')\n",
        "selected_df = selected_df.drop_duplicates(subset='description')\n",
        "selected_df = selected_df[selected_df['description'].notnull() & (selected_df['description'] != '')]\n",
        "selected_df = selected_df.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "EzmAJSJ5WNsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text preprocessing"
      ],
      "metadata": {
        "id": "djnjFdgFokYi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper function"
      ],
      "metadata": {
        "id": "vXxY4cJ9pXvx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming and Lemmatization\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "# Create a set of English words\n",
        "english_words = set(words.words())\n",
        "english_words = {item.lower() for item in english_words}\n",
        "spam_words = ['viral', 'trend', 'xuhuong', 'video', 'fyp', 'foryou', 'foryoupage', 'fypviral', 'fypage', 'tiktok', 'fypforyou', 'fouryou', 'fyppppppppppppppppppppppp', 'viralvideo',\n",
        "               'foryourpage', 'pov', 'trending',  'capcut', 'viraltiktok', 'new', 'сериал', 'bio', 'link', 'foryoupageofficial', 'fypp', 'xuhuongtiktok', 'xybca', 'fordig', 'youtube', 'name', 'edit', 'edits', 'xyzbca', 'know', 'part',  'back']\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
        "\n",
        "def filtering(description):\n",
        "    def is_valid_word(word):\n",
        "        return word.lower() in english_words\n",
        "    words = nltk.word_tokenize(description)\n",
        "    words = [w for w in words if w not in stopwords and w not in spam_words] # filter spam words\n",
        "    # Apply lemmatization technique\n",
        "    processed_words =[lemmatizer.lemmatize(word) for word in words]\n",
        "    # Filter out non-English words\n",
        "    filtered_words = [word for word in processed_words if is_valid_word(word)]\n",
        "    return ' '.join(processed_words)\n",
        "\n",
        "def remove_emoji(text):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                                u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                                u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                                u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                                u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                                u\"\\U00002702-\\U000027B0\"  # dingbats\n",
        "                                u\"\\U000024C2-\\U0001F251\"  # enclosed characters\n",
        "                                u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
        "                                u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
        "                                u\"\\U0001F700-\\U0001F77F\"  # Alchemical Symbols\n",
        "                                u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
        "                                u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
        "                                u\"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
        "                                u\"\\U00002700-\\U000027BF\"  # Dingbats\n",
        "                                u\"\\U0001F1E6-\\U0001F1FF\"  # Regional Indicator Symbols\n",
        "                                u\"\\U0001F004\"             # Mahjong Tile Red Dragon\n",
        "                                u\"\\U0001F0CF\"             # Playing Card Black Joker\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)\n",
        "\n",
        "def text_cleaning_custom(text):\n",
        "    # Tokenize the text into words\n",
        "    word_list = re.findall(r'\\b\\w+\\b', text)\n",
        "    # Filter out single-double-triple characters words\n",
        "    filtered_words = [word for word in word_list if len(word) > 3]\n",
        "    # Join the filtered words back into a single string\n",
        "    filtered_text = ' '.join(filtered_words)\n",
        "\n",
        "    return filtered_text\n",
        "\n",
        "def text_preprocess (df):\n",
        "    # Remove punctuations from the Description column\n",
        "    punctuations = '''()-[]{};:\"\\,<>./?@#$%^&*£!~ '''\n",
        "    text_list = []\n",
        "    for text in df['description']:\n",
        "        sentence = \"\"\n",
        "        for char in text:\n",
        "            if (char not in punctuations):\n",
        "                sentence = sentence + char\n",
        "            else:\n",
        "                sentence = sentence + \" \"\n",
        "        # Removing number\n",
        "        pattern = r'\\d+'\n",
        "        sentence = re.sub(pattern, '', sentence)\n",
        "\n",
        "        sentence = remove_emoji(sentence)\n",
        "        sentence = text_cleaning_custom(sentence)\n",
        "\n",
        "        # Mutuating the text cleaning column as sentence\n",
        "        text_list.append(sentence.lower())\n",
        "    # Apply the function to the Description column and create the Tokens column\n",
        "    df['token'] = text_list\n",
        "    df['sentence'] = df['token'].apply(filtering)\n",
        "\n",
        "def calculate_confidence_interval_95(dataframe, column):\n",
        "    mean = dataframe[column].mean()\n",
        "    sem = stats.sem(dataframe[column])\n",
        "\n",
        "    ci = stats.t.interval(0.95, len(dataframe[column])-1, loc=mean, scale=sem)\n",
        "    return ci\n",
        "\n",
        "# Unigram\n",
        "def get_top_n_words(corpus, n=10):\n",
        "    vec = CountVectorizer(stop_words='english').fit(corpus)\n",
        "    bag_of_words = vec.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0)\n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
        "    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
        "    return words_freq[:n]\n",
        "\n",
        "# Bigram\n",
        "def get_top_n_bigrams(corpus, n=10):\n",
        "    vec = CountVectorizer(ngram_range=(2, 2), stop_words='english').fit(corpus)\n",
        "    bag_of_words = vec.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0)\n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
        "    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
        "    return words_freq[:n]\n",
        "\n",
        "def plot_horizontal_bar_chart(data, title):\n",
        "    words, freqs = zip(*data)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.barh(words, freqs, color='#5ca7f7')\n",
        "    plt.xlabel('Frequency')\n",
        "    plt.ylabel('Words')\n",
        "    plt.gca().invert_yaxis()  # Invert y-axis to have the highest frequency on top\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "def print_top_terms_per_k_mean_cluster(tfidf_df, terms, num_terms=10):\n",
        "    cluster_centers = kmeans.cluster_centers_\n",
        "    original_centroids = pca.inverse_transform(cluster_centers)  # Transform centroids back to original feature space\n",
        "    terms = vectorizer.get_feature_names_out()\n",
        "\n",
        "    for i, centroid in enumerate(original_centroids):\n",
        "        top_indices = centroid.argsort()[-num_terms:][::-1]  # Get indices of top terms\n",
        "        top_terms = [terms[idx] for idx in top_indices]\n",
        "        print(f\"Cluster {i + 1}:\")\n",
        "        print(\", \".join(top_terms))\n",
        "        print(\"\\n\")\n",
        "\n",
        "# Hierarchical\n",
        "def compute_wss_hierarchical(data, labels):\n",
        "    wss = 0\n",
        "    for label in np.unique(labels):\n",
        "        cluster_data = data[labels == label]\n",
        "        centroid = cluster_data.mean(axis=0)\n",
        "        wss += ((cluster_data - centroid) ** 2).sum()\n",
        "    return wss\n",
        "\n",
        "def print_top_terms_per_hierarchical_cluster(tfidf_df, terms, num_terms=10):\n",
        "    cluster_terms = {}\n",
        "    for cluster in range(1, max_clusters + 1):\n",
        "        cluster_data = tfidf_df[tfidf_df['cluster'] == cluster].drop(columns=['cluster'])\n",
        "        cluster_mean = cluster_data.mean(axis=0)\n",
        "        top_terms = cluster_mean.sort_values(ascending=False).head(num_terms).index.tolist()\n",
        "        cluster_terms[cluster] = top_terms\n",
        "        print(f\"Cluster {cluster}:\")\n",
        "        print(\", \".join(top_terms))\n",
        "        print(\"\\n\")\n",
        "    return cluster_terms\n",
        "\n",
        "def tokenize_LDA_model(sentences):\n",
        "    for sentence in sentences:\n",
        "        # deacc=True removes punctuations\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
        "\n",
        "\n",
        "def make_bigrams(texts, bigram_mod):\n",
        "    return [bigram_mod[doc] for doc in texts]\n",
        "\n",
        "def make_trigrams(texts, trigram_mod, bigram_mod):\n",
        "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
        "\n",
        "def lemmatization(texts, allowed_postags=['NOUN']):\n",
        "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
        "    texts_out = []\n",
        "    for sent in texts:\n",
        "        doc = nlp(\" \".join(sent))\n",
        "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "    return texts_out\n",
        "\n",
        "def compute_coherence_values(corpus, dictionary, text, k, a, b):\n",
        "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
        "                                           id2word=dictionary,\n",
        "                                           num_topics=k,\n",
        "                                           random_state=100,\n",
        "                                           chunksize=100,\n",
        "                                           passes=5,\n",
        "                                           alpha=a,\n",
        "                                           eta=b)\n",
        "\n",
        "    coherence_model_lda = CoherenceModel(model=lda_model, texts=text, dictionary=id2word, coherence='c_v')\n",
        "\n",
        "    return coherence_model_lda.get_coherence()\n",
        "\n",
        "def get_dominant_topic(lda_model, corpus):\n",
        "    dominant_topics = []\n",
        "    for doc_bow in corpus:\n",
        "        doc_topics = lda_model.get_document_topics(doc_bow)\n",
        "        dominant_topic = max(doc_topics, key=lambda x: x[1])[0]\n",
        "        dominant_topics.append(dominant_topic + 1)\n",
        "    return dominant_topics\n",
        "\n",
        "# Sentiment analysis\n",
        "def classify_sentiment(compound):\n",
        "    if compound > 0:\n",
        "      return \"Positive\"\n",
        "    elif compound < 0:\n",
        "      return \"Negative\"\n",
        "    else:\n",
        "      return \"Neutral\"\n",
        "\n",
        "# Visualization insights\n",
        "def convert_num_cluster_to_text(num):\n",
        "  match num:\n",
        "    case 1:\n",
        "      return \"Entertainment\"\n",
        "    case 2:\n",
        "      return \"Movie and Music\"\n",
        "    case 3:\n",
        "      return \"Travel\"\n",
        "    case 4:\n",
        "      return \"Lifestyle\"\n",
        "    case 5:\n",
        "      return \"Hobby\"\n",
        "    case 6:\n",
        "      return \"Sport and Comedy\"\n",
        "    case 7:\n",
        "      return \"Social Media\"\n",
        "    case 8:\n",
        "      return \"Humour\"\n",
        "\n",
        "def calculate_category_distribution(df, total_count, attribute):\n",
        "  result = df.groupby(attribute).size().reset_index(name='Amount')\n",
        "  result['% Amount'] = (result['Amount'] / total_count) * 100\n",
        "  result['% Amount'] = result['% Amount'].round(2)\n",
        "  result = result.sort_values(by='Amount', ascending=False)\n",
        "  return result\n",
        "\n",
        "def find_popular_distribution(df, attribute, label_attribute, n):\n",
        "  top_100 = df.sort_values(attribute, ascending=False).head(n)\n",
        "  result_df = top_100.groupby('Category').agg({attribute: ['count', 'mean']}).reset_index()\n",
        "  result_df.columns = ['Category', 'AMOUNT', label_attribute]\n",
        "  result_df = result_df.sort_values('AMOUNT', ascending=False)\n",
        "\n",
        "  return result_df"
      ],
      "metadata": {
        "id": "rZWPxtdypPtk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "muslim_keywords = [\n",
        "    \"allah\", \"muslimtiktok\", \"اسلام\", \"مسلم\", \"اسلامی\", \"قرآن\",\n",
        "    \"حديث\", \"الحمدلله\", \"الله\", \"الدين\", \"محمد\", \"ابو_زير\", \"الدعاء\",\n",
        "    \"مسجد\", \"الحجاب\", \"الصلاة\", \"الصوم\", \"الجهاد\", \"رمضان\",\n",
        "    \"مكة\", \"المدينة\", \"الشهادة\", \"الزكاة\", \"الإسلام\", \"القرآن\",\n",
        "    \"الدعوة\", \"شريعة\", \"أمة\", \"خليفة\", \"الفتوى\", \"الحديث\",\n",
        "    \"عمرة\", \"حج\", \"الحلال\", \"الحرام\", \"النبى\", \"الصحابة\", \"الجنة\",\n",
        "    \"جهنم\", \"يوم_القيامة\", \"شهيد\", \"الخلافة\", \"المسلمون\"\n",
        "]\n",
        "\n",
        "selected_df = selected_df[~selected_df['description'].apply(lambda x: any(word in x.lower() for word in muslim_keywords))]\n",
        "text_preprocess(selected_df)\n",
        "selected_df = selected_df[selected_df['sentence'].notnull() & (selected_df['sentence'] != '')]"
      ],
      "metadata": {
        "id": "ACTFuBSoxo7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "EKWQI2kpkLnQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(selected_df.shape)\n",
        "print(calculate_confidence_interval_95(selected_df, 'playCount'))\n",
        "print(calculate_confidence_interval_95(selected_df, 'likeCount'))\n",
        "print(calculate_confidence_interval_95(selected_df, 'commentCount'))\n",
        "print(calculate_confidence_interval_95(selected_df, 'collectCount'))\n",
        "print(calculate_confidence_interval_95(selected_df, 'shareCount'))\n",
        "\n",
        "# Statistical summary of the dataset\n",
        "selected_df.describe().T\n",
        "\n",
        "num_cols = selected_df.select_dtypes(include=np.number).columns.tolist()\n",
        "#num_cols.remove(['id'])\n",
        "\n",
        "for col in num_cols:\n",
        "  plt.figure(figsize = (15,4))\n",
        "  plt.subplot(1,2,1)\n",
        "  selected_df[col].hist(grid=False)\n",
        "  plt.ylabel('Count')\n",
        "  plt.subplot(1,2,2)\n",
        "  sns.boxplot(x=df[col])\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "szs_ub8SxQiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word Cloud Visualization"
      ],
      "metadata": {
        "id": "zviyjf6gOQPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "long_string_without_cleaning = ','.join(list(selected_df['description'].values))\n",
        "wordcloud = WordCloud(background_color= \"white\",\n",
        "                      max_words= 5000, contour_width= 3,\n",
        "                      contour_color= 'steelblue')\n",
        "\n",
        "wordcloud.generate(long_string_without_cleaning)\n",
        "# save word clound to jpg image\n",
        "wordcloud.to_file(\"wordcloud_v1.jpg\")"
      ],
      "metadata": {
        "id": "MEOAEKP4ZrAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "long_string = ','.join(list(selected_df['sentence'].values))\n",
        "\n",
        "wordcloud = WordCloud(background_color= \"white\",\n",
        "                      max_words= 5000, contour_width= 3,\n",
        "                      contour_color= 'steelblue')\n",
        "\n",
        "wordcloud.generate(long_string)\n",
        "# save word clound to jpg image\n",
        "wordcloud.to_file(\"wordcloud_v2.jpg\")"
      ],
      "metadata": {
        "id": "2KezZ1wmXiGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unigram and Bigram Analysis"
      ],
      "metadata": {
        "id": "HyIl4ZofHg3L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_10_unigrams = get_top_n_words(selected_df['sentence'], 10)\n",
        "top_10_bigrams = get_top_n_bigrams(selected_df['sentence'], 10)\n",
        "\n",
        "# Print top 10 unigrams\n",
        "print(\"Top 10 Unigrams:\")\n",
        "for word, freq in top_10_unigrams:\n",
        "    print(f\"{word}: {freq}\")\n",
        "\n",
        "# Print top 10 bigrams\n",
        "print(\"\\nTop 10 Bigrams:\")\n",
        "for word, freq in top_10_bigrams:\n",
        "    print(f\"{word}: {freq}\")\n",
        "\n",
        "# Plot top 10 unigrams\n",
        "plot_horizontal_bar_chart(top_10_unigrams, 'Top 10 Unigrams')\n",
        "\n",
        "# Plot top 10 bigrams\n",
        "plot_horizontal_bar_chart(top_10_bigrams, 'Top 10 Bigrams')"
      ],
      "metadata": {
        "id": "dL3LmPFJ0_6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Partitional clustering models\n",
        "- Build model\n",
        "- Evaluation model\n",
        "- Interprete the topic modelling (Word list)"
      ],
      "metadata": {
        "id": "7JOpFbpj5lIn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF transformation to distance matrix"
      ],
      "metadata": {
        "id": "O2M6DjR6v2CQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = selected_df['sentence'].values.astype(\"U\")\n",
        "# Convert text to distance matrix\n",
        "# Modify the TfidfVectorizer to include N-grams (bigrams and trigrams)\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1, 3), max_features=10000)  # You can adjust max_features as needed\n",
        "\n",
        "features = vectorizer.fit_transform(documents)\n",
        "features_dense = features.toarray()"
      ],
      "metadata": {
        "id": "rRVytz4cxx2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Principle Component Analysis"
      ],
      "metadata": {
        "id": "8NaFgK3Dmt3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dimensionality Reduction\n",
        "pca = PCA(n_components=7)\n",
        "features_reduced = pca.fit_transform(features_dense)\n",
        "\n",
        "plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_)\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Explained Variance Ratio')\n",
        "plt.title('Scree Plot')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "X2HpZUmnsbJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K-mean text clustering"
      ],
      "metadata": {
        "id": "oxhHmG_SlkkB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Do the hyperparametes tuning\n",
        "- to find the optimal k-value"
      ],
      "metadata": {
        "id": "qCPqB_eFVHDB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "K = range(2, 20)\n",
        "wss = []\n",
        "\n",
        "for i in K:\n",
        "  model = KMeans(\n",
        "      n_clusters=i,\n",
        "      init= \"k-means++\",\n",
        "      random_state= 200\n",
        "  )\n",
        "\n",
        "  labels= model.fit(features_reduced).labels_\n",
        "\n",
        "  wss_iter = model.inertia_\n",
        "  wss.append(wss_iter)\n",
        "\n",
        "metrics_centers = pd.DataFrame({\n",
        "    'Clusters': K,\n",
        "    'WSS': wss,\n",
        "})\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(18, 5))\n",
        "\n",
        "# Plot the elbow method (WSS)\n",
        "sns.lineplot(ax=ax, x='Clusters', y='WSS', data=metrics_centers, marker='+')\n",
        "ax.set_title('Within-Cluster Sum of Squares (WSS)')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VPiojkaU9FLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform K-Mean Clustering with k Clusters.\n",
        "# Select the optimal k-value = 11, considering by elbow-method at WSS metrics\n",
        "k = 11\n",
        "kmeans = KMeans(n_clusters= k, init= \"k-means++\")\n",
        "\n",
        "labels= kmeans.fit(features_reduced).labels_\n",
        "\n",
        "silhouette_score = metrics.silhouette_score(\n",
        "      features_reduced,\n",
        "      labels,\n",
        "      metric= 'euclidean',\n",
        "      sample_size= len(selected_df),\n",
        "      random_state= 200\n",
        "  )\n",
        "\n",
        "db_index = metrics.davies_bouldin_score(features_reduced, labels)\n",
        "print(silhouette_score)\n",
        "print(db_index)\n",
        "\n",
        "selected_df['k_mean_cluster'] = kmeans.labels_\n",
        "\n",
        "# Get feature names (terms) from the vectorizer\n",
        "terms = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Create a DataFrame from the original TF-IDF features (before PCA)\n",
        "tfidf_df = pd.DataFrame(features_dense, columns=terms)\n",
        "tfidf_df['cluster'] = kmeans.labels_\n",
        "\n",
        "print_top_terms_per_k_mean_cluster(tfidf_df, terms)"
      ],
      "metadata": {
        "id": "ZT5NZBR2HxKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the 3D k-means clustering distribution of each cluster\n",
        "\n",
        "fig = plt.figure(figsize=(10, 10))\n",
        "ax = fig.add_subplot(111, projection='3d', elev=40, azim=80)\n",
        "\n",
        "# Scatter plot\n",
        "sc = ax.scatter(selected_df['playCount'], selected_df['likeCount'], selected_df['commentCount'],\n",
        "                c=selected_df['k_mean_cluster'], cmap='rainbow')\n",
        "\n",
        "ax.set_xlabel(\"Play Count\", labelpad=10)\n",
        "ax.set_ylabel(\"Like Count\", labelpad=10)\n",
        "ax.set_zlabel(\"Comment Count\", labelpad=20)\n",
        "\n",
        "ax.set_facecolor('white')\n",
        "plt.title(\"Tiktok Video Clustering using K Means\", fontsize=14)\n",
        "\n",
        "# Optional: Add a color bar\n",
        "plt.colorbar(sc)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qk6yEzSyFN3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hierarchical model"
      ],
      "metadata": {
        "id": "qae8h5Do21oR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ward_cluster = linkage(features_reduced, method= 'ward')"
      ],
      "metadata": {
        "id": "v1AfiSfzesPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot dendrogram chart\n",
        "plt.figure(figsize=(10,5))\n",
        "dendrogram(ward_cluster, labels=documents, leaf_rotation=90, leaf_font_size=10)\n",
        "plt.title('Hierarchical Clustering Dendrogram')\n",
        "plt.xlabel('Document')\n",
        "plt.ylabel('Distance')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xo7cZkBSUBx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run Hierarchical algorithm\n",
        "\n",
        "K = range(2, 20)\n",
        "wss = []\n",
        "\n",
        "for max_clusters in K:\n",
        "  hirarchical_clusters = fcluster(ward_cluster, max_clusters, criterion= 'maxclust')\n",
        "\n",
        "  wss.append(compute_wss_hierarchical(features_reduced, hirarchical_clusters))\n",
        "\n",
        "metrics_centers = pd.DataFrame({\n",
        "    'Clusters': K,\n",
        "    'WSS': wss\n",
        "})\n",
        "\n",
        "# plot elbow method (WSS)\n",
        "fig, ax = plt.subplots(1, 1, figsize=(18, 5))\n",
        "sns.lineplot(ax=ax, x='Clusters', y='WSS', data=metrics_centers, marker='+')\n",
        "ax.set_title('Within-Cluster Sum of Squares (WSS)')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Wz0IZYYgmtpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_clusters = 9\n",
        "hirarchical_clusters = fcluster(ward_cluster, max_clusters, criterion= 'maxclust')\n",
        "\n",
        "sil_score = silhouette_score(features_reduced, hirarchical_clusters)\n",
        "# 'numpy.float64' object is not callable if you found this error, please move back to run import libraries again (It may lose some required tools)\n",
        "db_score = davies_bouldin_score(features_reduced, hirarchical_clusters)\n",
        "\n",
        "print(sil_score)\n",
        "print(db_score)\n",
        "selected_df['hirarchical_cluster'] = hirarchical_clusters\n",
        "\n",
        "# Get feature names (terms) from the vectorizer\n",
        "terms = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Create a DataFrame from the original TF-IDF features (before PCA)\n",
        "tfidf_df = pd.DataFrame(features_dense, columns=terms)\n",
        "tfidf_df['cluster'] = hirarchical_clusters\n",
        "\n",
        "cluster_terms = print_top_terms_per_hierarchical_cluster(tfidf_df, terms)"
      ],
      "metadata": {
        "id": "g3OabYuWZmN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LDA Model"
      ],
      "metadata": {
        "id": "5hre_KrkJIgO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_words = selected_df['sentence'].values.tolist()\n",
        "data_words = list(tokenize_LDA_model(data_words)) # tokenization\n",
        "\n",
        "# Build the bigram and trigram models\n",
        "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
        "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)\n",
        "\n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
        "\n",
        "# Form Trigrams\n",
        "data_words = make_trigrams(data_words, trigram_mod, bigram_mod)\n",
        "\n",
        "# Create Dictionary\n",
        "texts = data_words\n",
        "id2word = corpora.Dictionary(texts)\n",
        "\n",
        "# Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in texts]"
      ],
      "metadata": {
        "id": "frUdE_rXblpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter Tuning\n",
        "## C_v\n",
        "- measure is based on a sliding window, one-set segmentation of the top words and an indirect confirmation measure that uses normalized pointwise mutual information (NPMI) and the cosine similarity"
      ],
      "metadata": {
        "id": "E8QZGU3lkxrc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Topics range\n",
        "min_topics = 8\n",
        "max_topics = 9\n",
        "step_size = 1\n",
        "topics_range = range(min_topics, max_topics, step_size)\n",
        "\n",
        "alpha = list(np.arange(0.01, 1, 0.1))\n",
        "beta = list(np.arange(0.01, 1, 0.1))\n",
        "\n",
        "# Validation sets\n",
        "num_of_docs = len(corpus)\n",
        "corpus_sets = [corpus]\n",
        "\n",
        "model_results = {'Topics': [],\n",
        "                 'Alpha': [],\n",
        "                 'Beta': [],\n",
        "                 'Coherence': []\n",
        "                }\n",
        "data_lemmatized = lemmatization(data_words, allowed_postags=['NOUN'])\n",
        "# Can take a long time to run\n",
        "if 1 == 1:\n",
        "    for i in range(len(corpus_sets)):\n",
        "        # iterate through number of topics\n",
        "        for k in topics_range:\n",
        "            # iterate through alpha values\n",
        "            for a in alpha:\n",
        "                # iterare through beta values\n",
        "                for b in beta:\n",
        "                    # get the coherence score for the given parameters\n",
        "                    cv = compute_coherence_values(corpus=corpus_sets[i], dictionary=id2word, text=data_lemmatized, k=k, a=a, b=b)\n",
        "                    model_results['Topics'].append(k)\n",
        "                    model_results['Alpha'].append(a)\n",
        "                    model_results['Beta'].append(b)\n",
        "                    model_results['Coherence'].append(cv)"
      ],
      "metadata": {
        "id": "Wx9JrMKolBBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# export the experimental LDA results\n",
        "file_path = 'coherance_results.xlsx'\n",
        "pd.DataFrame(model_results).to_excel(file_path, index=False)"
      ],
      "metadata": {
        "id": "VdXrEkTQ5Sd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the some of LDA modelling to see the best performance parameter\n",
        "plt.style.use(\"_classic_test_patch\")\n",
        "x= [2,3,4,5,6,7,8,9]\n",
        "y = [0.47502992152505985,\n",
        "  0.4739038870968999,\n",
        "  0.5191712392844225,\n",
        "  0.5696449051925503,\n",
        "  0.5540307696200729,\n",
        "  0.5702856314238206,\n",
        "  0.5962194674972741,\n",
        "  0.5820308009596259]\n",
        "\n",
        "plt.plot(x, y, marker='o', color='#2C15FF')\n",
        "plt.xlabel('the number of topics')\n",
        "plt.ylabel('Coherence Score')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hl3MQr9BhHiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build LDA with optimal value"
      ],
      "metadata": {
        "id": "-uV6FZBnxHLa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_topics = 8\n",
        "\n",
        "# Build LDA model\n",
        "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
        "                                           id2word=id2word,\n",
        "                                           num_topics=num_topics,\n",
        "                                           random_state=100,\n",
        "                                           chunksize=100,\n",
        "                                           passes=10,\n",
        "                                           alpha=0.81,\n",
        "                                           eta=0.01)\n",
        "\n",
        "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_words, dictionary=id2word, coherence='c_v')\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print('Coherence Score: ', coherence_lda)\n",
        "# Get topic distribution for each document\n",
        "doc_lda = lda_model.get_document_topics(corpus, minimum_probability=0)\n",
        "selected_df['LDA_cluster'] = get_dominant_topic(lda_model, corpus)\n",
        "# Plot the distribution of each topic of LDA model\n",
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word, mds='mmds', R=10)\n",
        "vis"
      ],
      "metadata": {
        "id": "EFWzcLGWiDT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print dominant topic with topic_keywords of LDA model\n",
        "data_dict = {'dominant_topic':[], 'perc_contribution':[], 'topic_keywords':[]}\n",
        "\n",
        "for i, row in enumerate(lda_model[corpus]):\n",
        "    row = sorted(row, key=lambda x: x[1], reverse=True)\n",
        "    for j, (topic_num, prop_topic) in enumerate(row):\n",
        "        wp = lda_model.show_topic(topic_num)\n",
        "        topic_keywords = \", \".join([word for word, prop in wp])\n",
        "        data_dict['dominant_topic'].append(int(topic_num))\n",
        "        data_dict['perc_contribution'].append(round(prop_topic, 3))\n",
        "        data_dict['topic_keywords'].append(topic_keywords)\n",
        "        break\n",
        "\n",
        "df_topics = pd.DataFrame(data_dict)\n",
        "print(df_topics)\n",
        "\n",
        "# mapping the video category to dominant_topic (LDA_cluster)\n",
        "selected_df[\"Category\"] = selected_df[\"LDA_cluster\"].apply(convert_num_cluster_to_text)"
      ],
      "metadata": {
        "id": "87uXSR3YcZlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis"
      ],
      "metadata": {
        "id": "bjYixFDLz4OX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sia = SentimentIntensityAnalyzer()\n",
        "selected_df['token'] = selected_df['sentence'].apply(lambda x: nltk.word_tokenize(x))\n",
        "\n",
        "res = {}\n",
        "for i, row in tqdm(selected_df.iterrows(), total=len(selected_df)):\n",
        "  text = row['sentence']\n",
        "  myid = row['id']\n",
        "  res[myid] = sia.polarity_scores(text)\n",
        "\n",
        "vaders = pd.DataFrame(res).T\n",
        "vaders = vaders.reset_index().rename(columns= {'index': 'id'})\n",
        "vaders = vaders.merge(selected_df, how= 'left')\n",
        "# apply the emotional compund score to the dataframe\n",
        "vaders['Sentiment'] = vaders['compound'].apply(classify_sentiment)"
      ],
      "metadata": {
        "id": "o70n3Ooa0C0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "total_count= len(selected_df)\n",
        "result = selected_df['k_mean_cluster'].value_counts().reset_index(name='Amount')\n",
        "result['% Amount'] = (result['Amount'] / total_count) * 100\n",
        "result['% Amount'] = result['% Amount'].round(2)\n",
        "result"
      ],
      "metadata": {
        "id": "bkk8O9HHGOzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the proportion of each emotional type (positive, neutral, negative)\n",
        "total_count= len(vaders)\n",
        "result = vaders['Sentiment'].value_counts().reset_index(name='Amount')\n",
        "result['% Amount'] = (result['Amount'] / total_count) * 100\n",
        "result['% Amount'] = result['% Amount'].round(2)\n",
        "\n",
        "fig = px.bar(result, x=\"Sentiment\", y=\"% Amount\", color= \"Amount\", color_continuous_scale='GnBu')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "kcq2FCcTfM79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visual the insights"
      ],
      "metadata": {
        "id": "4RZlu9tlhupE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_rows = len(selected_df)\n",
        "category_dist_fig = calculate_category_distribution(selected_df, total_rows, 'Category')\n",
        "\n",
        "fig = px.bar(category_dist_fig, x=\"% Amount\", y=\"Category\", color=\"Amount\", orientation='h', color_continuous_scale='GnBu')\n",
        "fig.update_layout(yaxis_title=\"Category\")\n",
        "fig.show()\n",
        "\n",
        "popular_distribution_top_100 = find_popular_distribution(selected_df, 'playCount', 'Average Views', 100)\n",
        "fig = px.bar(popular_distribution_top_100, x=\"AMOUNT\", y=\"Category\", color=\"Average Views\", orientation='h', color_continuous_scale='Emrld')\n",
        "fig.update_layout(yaxis_title=\"Category\")\n",
        "fig.show()\n",
        "\n",
        "engagement_distribution_top_100 = find_popular_distribution(selected_df, 'commentCount', 'Average Comments', 100)\n",
        "fig = px.bar(engagement_distribution_top_100, x=\"AMOUNT\", y=\"Category\", color=\"Average Comments\", orientation='h', color_continuous_scale='Purp')\n",
        "fig.update_layout(yaxis_title=\"Category\")\n",
        "fig.show()\n",
        "\n",
        "# The percentage of the sentiment score of each category\n",
        "total_count= len(vaders)\n",
        "result = vaders.groupby(['Category', 'Sentiment']).size().reset_index(name='Amount')\n",
        "result['% Amount'] = (result['Amount'] / total_count) * 100\n",
        "result['% Amount'] = result['% Amount'].round(2)\n",
        "\n",
        "fig = px.bar(result, x=\"Category\", y=\"% Amount\", color=\"Sentiment\", text=\"Amount\")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "OwGQFgRIiOiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the top 100 videos based on views\n",
        "n = 100\n",
        "result = selected_df[['id', 'description', 'playCount', 'likeCount', 'Category']].rename(columns={\n",
        "      'description': 'Video Description',\n",
        "      'playCount': 'Views',\n",
        "      'likeCount': 'Likes'\n",
        "  }).sort_values(by='Views', ascending=False).head(n)\n",
        "\n",
        "result"
      ],
      "metadata": {
        "id": "3rtVTwb6omc-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}